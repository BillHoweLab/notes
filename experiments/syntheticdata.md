# Synthetic Data Generation

## Questions

Two separate research questions:
1. How do smaller LLMs (open to which one, helpful if training data distribution fully described) compare to GPT-4's ability to synthesize data given a provided data distribution for tabular data?
2. For some space of data described *semanticly*, can we understand the difference between LLM-generated datasets compared to the datasets associated with the same semantic description in academic research?

## Description

We want to understand the extent to which a LLM can synthesize data in a representative way across a semantically defined data distribution. 

## Design

For RQ #1:
- 

For RQ #2:
- 

## Results

No results yet.

## References

- Torralba and Efros (2011). An unbiased look at dataset bias. CVPR. https://ieeexplore.ieee.org/document/5995347.
- Borisov et al. (2023). Language Models are Realistic Tabular Data Generators. ICLR. (arXiv: https://arxiv.org/abs/2210.06280) (OpenReview: https://openreview.net/forum?id=cEygmQNOeI)
