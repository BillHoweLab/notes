# Synthetic Data Generation

## Questions

Two separate research questions:
1. What can we learn from replicating research on synthesizing tabular data, e.g., Borisov et al. 2023? How does this compare on smaller open source LLMs versus GPT-4 and others? Finetuning or context window?
2. For some space of data described *semanticly*, can we understand the difference between LLM-generated datasets compared to the datasets associated with the same semantic description?

## Description

We want to understand the extent to which a LLM can synthesize data in a representative way across a semantically defined data distribution. 

## Design

For RQ #1:
- Calculate computational and financial costs
- Generate prompt templates for ingesting tabular data (one approach in Borisov et al.)
- Finetune models to an individual dataset
- Prompt and collect data using various temperatures, prompts, etc.
- Compare synthetic data metrics (ML efficiency, etc.) across datasets generated by models

For RQ #2 (very tentative and frankly, made up at this point):
- Develop a particular semantic definition of a space
- Collect datasets used in literature that invoke that keyword/phrase (data distribution #1)
- Generate prompt templates for generating synthetic data based on for semantic keywords/phrase and collect data (data distribution #2)
- 

## Results

No results yet.

## References

- Torralba and Efros (2011). An unbiased look at dataset bias. CVPR. https://ieeexplore.ieee.org/document/5995347.
- Borisov et al. (2023). Language Models are Realistic Tabular Data Generators. ICLR. (arXiv: https://arxiv.org/abs/2210.06280) (OpenReview: https://openreview.net/forum?id=cEygmQNOeI)
- Veselovsky et al. (2023) [preprint]. Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science. (arXiv: https://arxiv.org/pdf/2305.15041.pdf)